
# IR 风格相似度计算优化方案

> 目标：在你现有的「 **按年 npz + 稀疏向量 + 阈值求和** 」框架内，
>
> * **显著快过 SpGEMM**
> * **内存可控（不再 swap / 借用存储）**
> * **(x, y) 年份对只算一次**
> * **任意年份对完成即可 checkpoint、断点续跑**

本文把方案拆成四个可以**独立落地、逐步上线**的模块，并在最后给出一个 **完整工程级架构** 。

---

## 1️⃣ IR 风格候选生成：倒排索引 + 阈值前移剪枝

### 1.1 你当前方法的瓶颈

你现在的逻辑本质是：

```
S = Mx · My^T        # 稀疏矩阵乘
filter(S >= thr)
```

问题在于：

* **大量最终 < thr 的候选也被完整算出**
* 这些候选：
  * 被累加
  * 被写入稀疏结构
  * 再被丢弃

这一步在大规模年份（千万级非零）下是 **灾难性的** 。

IR（Information Retrieval）的核心思想是：

> **不要先生成完整相似度矩阵，而是在“生成过程中”就剪掉不可能达阈值的候选。**

---

### 1.2 倒排索引 + term-at-a-time 累加

对年份  **Y** ：

* 构建倒排索引（posting list）

```
for each term w:
    P[w] = [(doc_j, y_{j,w}), ...]
```

对年份 **X** 中的某个文档 `i`（其非零词为 `W_i`）：

```
score = {}   # dict: j -> partial similarity

for w in W_i:
    for (j, y_jw) in P[w]:
        score[j] += x_iw * y_jw

keep j where score[j] >= thr
```

特点：

* 数学结果 **等价于稀疏矩阵乘**
* 但你 **从不构造完整 S**
* 内存只与「当前文档候选数」有关

问题：posting 仍可能很长（高 df 词）。

---

### 1.3 关键提速：MaxScore / 上界剪枝（阈值前移）

#### 1.3.1 词项贡献上界

对年份  **Y** ，预先计算：

```
max_y[w] = max_j My[j, w]
```

对文档 `i` 的某个词 `w`：

```
UB_w(i) = x_{i,w} * max_y[w]
```

这是  **该词对任何候选文档的最大可能贡献** 。

---

#### 1.3.2 剩余上界与提前淘汰

将文档 `i` 的词按 `UB_w(i)`  **从大到小排序** ：

```
w1, w2, ..., wk
```

定义：

```
R_k(i) = sum_{t > k} UB_{w_t}(i)
```

在累加过程中，对某个候选 `j`：

```
if score[j] + R_k(i) < thr:
    delete j from score
```

含义：

* 即便把**剩余所有词的最大可能贡献都加上**
* 该候选也**永远达不到阈值**

➡️  **可以立刻丢弃，不再更新它** 。

---

#### 1.3.3 为什么这一步是数量级提速

* 多数候选：
  * 在前 3–10 个高权重词后
  * 就已确定「不可能 ≥ thr」
* 它们：
  * **不会再参与后续长 posting 的遍历**

👉 这一步通常带来 **10×–100×** 的候选更新数下降。

> 真正的提速来自  **“不算”** ，而不是算得更快。

---

### 1.4 实用补充技巧（工程友好）

#### A. 先处理短 posting 的词

* posting 短 → 稀有词 → 区分度高
* 候选集合起步更小
* 后续长 posting 只更新少量候选

可与 MaxScore 结合使用。

#### B. 两阶段候选（可选近似）

* Stage 1：
  * 只用前 L 个高权重词
  * 生成候选，截断到最多 C 个
* Stage 2：
  * 在候选集上精算

适合极端长尾文档。

---

## 2️⃣ 分块运算：把内存压下去，时间几乎不变

### 2.1 内存爆炸的根因

* 一次性算 `Mx · My^T`
* 中间候选 / 输出结构巨大
* 即使 IR，也可能出现「候选字典峰值过大」

---

### 2.2 按 X 的行分块（强烈推荐）

将年份 **X** 的文档分块：

```
X = [block_1, block_2, ...]
block size = 5k / 10k / 20k
```

对每个 block：

```
for doc i in block:
    IR accumulate against full Y
    write result
    clear score dict
```

优点：

* My 的倒排索引 **只建一次**
* 峰值内存 ∝ block size
* cache locality 更好
* 时间几乎不变，常常更快

👉 这是**最稳妥、性价比最高**的分块方式。

---

### 2.3 为什么不推荐按词分块

* 需要跨 batch 累加 score
* 逻辑复杂、bug 风险高
* 实测收益不如行分块稳定

---

## 3️⃣ (x, y) 只算一次，同时更新两个方向

### 3.1 对称性的利用

若相似度是 dot / cosine（对称）：

```
sim(x_i, y_j) = sim(y_j, x_i)
```

则年份对 `(x, y)`  **无需算两次** 。

---

### 3.2 在 IR 框架中怎么做

当处理 `x` 中的文档 `i`，得到 `score[j] (j ∈ y)`：

```
if score[j] >= thr:
    contrib_x[i] += score[j]
    contrib_y[j] += score[j]
```

需要维护两个数组：

```
contrib_x: shape (Nx,)
contrib_y: shape (Ny,)
```

➡️ 一次计算， **同时完成两个年份的贡献统计** 。

> 若 BS / FS 有前后方向区分，只是“写入哪一侧窗口”的问题。

---

## 4️⃣ pair 级落盘 checkpoint（核心工程能力）

### 4.1 每个年份对该存什么？

**不存 S，只存贡献向量：**

```
pair_contrib/x=2008_y=2012.npz
    contrib_x : (Nx,)
    contrib_y : (Ny,)
    meta      : thr, method_version, timestamp
```

特点：

* 文件小
* 完全满足后续聚合
* 天然支持断点续跑

---

### 4.2 聚合 BS / FS

定义示例：

```
BS(t) = sum_{y ∈ [t-5, t-1]} contrib_t_from_pair(t, y)
FS(t) = sum_{y ∈ [t+1, t+5]} contrib_t_from_pair(t, y)
```

做法：

* 只读需要的 pair 文件
* 取对应一侧 contrib
* 向量相加

---

### 4.3 Checkpoint 判定

最简单、最稳妥：

* **以文件存在性为准**
* 原子写入：
  * 先写临时文件
  * `rename()` 到最终路径

Pipeline 重启：

```
if pair_contrib/x=..._y=....npz exists:
    skip
```

---

## 5️⃣ 总体工程架构（可直接照着实现）

### 5.1 年份级预处理（一次性）

对每个年份 Y：

1. 加载剪枝后的 `My`
2. 构建倒排索引 `P[w]`
3. 计算 `max_y[w]`
4. 缓存 / 落盘（避免重复建）

---

### 5.2 主循环（年份对）

只枚举一次 `(x < y)`：

```
for (x, y) in year_pairs:
    if pair file exists:
        continue

    load Mx
    load index(My)

    alloc contrib_x, contrib_y

    for X_block in blocks(Mx):
        for doc i in X_block:
            IR accumulate + MaxScore prune
            for j with score[j] >= thr:
                contrib_x[i] += score[j]
                contrib_y[j] += score[j]

    atomic save pair_contrib
```

---

### 5.3 汇总阶段

```
for year t:
    BS_t = sum needed contribs
    FS_t = sum needed contribs
```

---

## 6️⃣ 推荐落地顺序（最低风险 → 最大收益）

1. **pair 级落盘 checkpoint** （不改算法也能做）
2. **(x, y) 对称复用** （立刻省 ~50% 时间）
3. **按行分块** （立刻解决内存问题）
4. **倒排 + MaxScore** （数量级提速的关键）

---

> 如果你愿意，下一步我可以：
>
> * 给你 **Python / NumPy / SciPy 级伪代码**
> * 或直接帮你设计 **npz / mmap / posting list 的数据结构**
> * 或帮你估算 **thr、block size、L/C 参数的经验范围**

这套方案在 IR / 大规模相似度 / 引文分析里是 **成熟且可复用的工程路径** 。



# 考虑性能


这套设计 **要想真做到 10× 级别提速，关键就在于：把“热循环”里的工作尽量放到 C 实现的数据结构/算子上，或至少用 NumPy 的连续数组减少 Python 层开销** 。我下面把设计里“哪些地方容易被 Python 拖慢”以及“怎么改成底层结构”讲清楚，并给你一个推荐落地路线（不要求你立刻上 C++，但能把 Python 开销压到可接受）。

---

## 1) 这套 IR 设计里，Python 最慢的点是什么？

主要有两个：

1. **候选 `scores` 用 Python dict 做累加**
   * 每次 `scores[j] += ...` 都是 Python 哈希查找 + 装箱/拆箱
   * 候选一多会非常慢
2. **遍历 posting list 的双层 for 循环如果在 Python 层跑**
   * `for term in doc_terms: for (j, w) in posting(term)`
   * 循环次数上亿时，Python 解释器会成为瓶颈

所以：

> 仅靠“算法更聪明”，但热循环仍是纯 Python dict + for，往往只能 2×～5×，很难 10×～100×。

---

# 2) 设计里哪些数据结构本身已经是 C 实现 / 低开销？

这部分你可以放心用：

### ✅ 稀疏矩阵（SciPy CSR/CSC）

* `scipy.sparse` 的存储（`indptr/indices/data`）都是 NumPy ndarray（C 连续内存）
* 切片、转换 CSR↔CSC 的大部分操作在 C/Fortran 层做

### ✅ postings 扁平化数组（推荐的 `ptr/docs_flat/vals_flat`）

* `ptr`, `docs_flat`, `vals_flat`, `max_y` 全是 NumPy ndarray
* 访问某个 term 的 posting 是：
  * `start=ptr[w]; end=ptr[w+1]`
  * `docs = docs_flat[start:end]`（视图）
  * `vals = vals_flat[start:end]`
* 这一步几乎全是 C 层切片，不慢

### ✅ memmap

* `np.memmap` 也是底层映射文件，切片仍走 C 路径
* 只要访问模式顺序性不错，性能很稳

也就是说： **数据的“承载”没问题，问题只在“更新 scores 的方式”** 。

---

# 3) `scores` 该怎么做，才能摆脱 Python dict？

这里给你 4 个等级，从“最容易落地”到“最强性能”。

## 方案 A（最容易落地，通常就能明显变快）：按块用 `np.bincount` 做聚合（C 实现）

对一个文档 i 的一个词 w：

* posting docs：`docs`（int32）
* posting vals：`vals`（float32）
* x 权重：`xw`（float32）

你想做的是对候选 doc j：

score[j]+=xw⋅vals[j]score[j] += xw \cdot vals[j]**score**[**j**]**+**=**x**w**⋅**v**a**l**s**[**j**]
你可以用：

<pre class="overflow-visible! px-0!" data-start="1375" data-end="1437"><div class="contain-inline-size rounded-2xl corner-superellipse/1.1 relative bg-token-sidebar-surface-primary"><div class="sticky top-[calc(--spacing(9)+var(--header-height))] @w-xl/main:top-9"><div class="absolute end-0 bottom-0 flex h-9 items-center pe-2"><div class="bg-token-bg-elevated-secondary text-token-text-secondary flex items-center gap-4 rounded-sm px-2 font-sans text-xs"></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre! language-python"><span><span>np.bincount(docs, weights=xw*vals, minlength=Ny)
</span></span></code></div></div></pre>

`bincount` 的累加在 C 层完成，速度非常快。

但问题是：`minlength=Ny` 会生成一个长度 Ny 的 dense 向量，Ny=20 万会很重（每词一次不行）。

解决办法： **块化 Ny** （见后面分块），或只对小 posting 使用 bincount。

> 实用策略：对 posting 长度 ≤ L（比如 2万）的词项，用 bincount；长 posting 走别的策略。

---

## 方案 B（推荐的“工程甜点位”）：用 `numba` 把热循环 JIT 到机器码

你仍然用 `ptr/docs_flat/vals_flat` + CSR 的 `indptr/indices/data`，但把“候选累加 + MaxScore 剪枝”写成 Numba 函数：

* Numba 会把 for 循环编译成接近 C 的速度
* 可以用 `numba.typed.Dict[int32, float32]` 作为 scores（仍是哈希，但在 JIT 内）
* 或者用 `int32` 数组当“稀疏 accumulator”（见方案 C）

这是**性价比最高**的路径：基本不改算法结构，但性能跃迁明显。

---

## 方案 C（性能更强，且更可控）：用“稀疏 accumulator”数组替代 dict

经典做法（IR/向量检索常用）：

* 维护一个 `acc` dense 数组：`acc = np.zeros(Ny, float32)`
* 再维护一个 `touched` 列表（int32），记录本 doc 被更新过的候选 j
* 每次更新：
  * 如果 `acc[j]==0`（或用一个 `mark[j]` 判断），先把 j append 到 touched
  * `acc[j] += delta`
* 最后只遍历 touched，筛 `acc[j]>=thr`，并把 touched 的 acc 清零

优点：

* 更新是数组索引写入（C/NumPy/Numba 都很快）
* touched 通常远小于 Ny（尤其你删 df + TopK 后）

缺点：

* `acc` 需要 Ny 长度的数组，Ny=20 万其实不大：
  * float32 20万 ≈ 0.8MB
  * 你甚至可以同时开两个数组（acc+mark）也不算大
* 真正大的是你若同时对很多 doc 开 acc（但我们是“每 doc 一个 acc”复用同一个数组）

> 这个方案配合 Numba，基本就进入“单机可跑”的正确轨道。

---

## 方案 D（最强但开发成本最高）：C++/Rust 扩展（pybind11 / pyo3）

如果你最后要榨干性能，可以把核心 pair 计算做成扩展模块：

* 输入：CSR（indptr/indices/data）+ postings（ptr/docs/vals）+ max_y + thr
* 输出：contrib_x / contrib_y

但一般你先上 **Numba + 稀疏 accumulator** 就够了。

---

# 4) “分块降低内存且时间几乎不变”在类型层面怎么保证？

你担心 Python 分块会不会变慢，关键在于：

* 分块只影响外层调度（Python 循环次数增加一点）
* **块内热循环必须走 Numba / C 层数组操作**
* postings 结构必须复用（不要每块 rebuild）

正确实现时：

* 外层 block 循环是 50 次/100 次这种量级，不是瓶颈
* 真正花时间的仍是块内候选累加（JIT 后非常快）

---

# 5) “pair 只算一次 + 双向更新”对效率的影响

这点在类型层面是纯数组写入：

* `contrib_x[i] += s`
* `contrib_y[j] += s`

只要 `contrib_*` 是 `np.ndarray(dtype=float32)`，这就是 C 连续内存，开销很低。

真正要避免的是：

* 把大量 (i,j,s) 存成 Python list 再处理（会炸）

---

# 6) 我建议你在设计中明确写进“性能实现要求”

为了让实现不会被 Python 拖死，我建议你把这几条当作硬约束：

1. postings 必须是  **扁平化 NumPy 数组** （ptr/docs/vals，int32/float32）
2. 热循环（候选累加 + 剪枝）必须使用：
   * **Numba JIT** （推荐）
   * 或最少用 `np` 的向量化 + 小 posting 的 `bincount`
3. `scores` 不允许用纯 Python dict 作为最终版本（可以先做正确性验证版）
4. 全部权重用 `float32`，doc id 用 `int32`
5. 所有落盘的 contrib 用 `float32`（够用、IO 更快）

---

# 7) 推荐落地路线（兼顾你现在工程推进速度）

1. **先做“正确性版本”** ：Python dict + postings（确认结果一致，能跑小年份）
2. 立刻切到  **Numba + 稀疏 accumulator（方案 C）** ：
   * 性能会有质变
3. 再加 MaxScore 剪枝（仍在 Numba 里）
4. 最后再做 mmap postings、并行调度
