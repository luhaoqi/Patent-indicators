下面是把你要做的 **4 个任务** + **性能实现要求（尽量用 C/Numba/NumPy 结构，避开 Python 低效热循环）** 汇总成的一份“最终实现方案”。你可以直接把它当成下一步改 `similarity.py` / pipeline 的蓝图。

---

## 总目标

把原先的  **SpGEMM：`S = Mx · My^T` + `threshold + sum`** ，替换为：

* **IR 倒排候选生成** （term-at-a-time）
* **MaxScore 无损阈值剪枝** （把阈值前移，减少无用候选更新）
* **按 X 文档分块** （控制内存峰值，避免构造 S）
* **(x,y) 只算一次、双向更新贡献向量** （省 50%）
* **pair 级落盘 checkpoint** （任意年份对完成即可存档、断点续跑）
* 关键热循环尽量在 **C/NumPy/Numba** 层跑（避免 Python dict / 大量 Python for）

---

# 最终输出与落盘结构（pair_contrib）

对每个年份对 `(x,y)`，只保存两个向量：

* `contrib_x`: shape `(Nx,)` float32

  x 年每篇文档来自 y 年所有 `sim>=thr` 邻居的相似度之和
* `contrib_y`: shape `(Ny,)` float32

  y 年每篇文档来自 x 年所有 `sim>=thr` 邻居的相似度之和

落盘路径（原子写入）：

<pre class="overflow-visible! px-0!" data-start="670" data-end="816"><div class="contain-inline-size rounded-2xl corner-superellipse/1.1 relative bg-token-sidebar-surface-primary"><div class="sticky top-[calc(--spacing(9)+var(--header-height))] @w-xl/main:top-9"><div class="absolute end-0 bottom-0 flex h-9 items-center pe-2"><div class="bg-token-bg-elevated-secondary text-token-text-secondary flex items-center gap-4 rounded-sm px-2 font-sans text-xs"></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre!"><span><span>artifacts/pair_contrib/x=</span><span>2008</span><span>_y=</span><span>2012.</span><span>npz
  - contrib_x (</span><span>float32</span><span>)
  - contrib_y (</span><span>float32</span><span>)
  - meta: thr, method_version, Nx, Ny, created_at
</span></span></code></div></div></pre>

> 不存 S，不存 4e8 nnz。你的内存问题和 IO 问题都会大幅缓解。

---

# 任务 1：IR 风格候选生成（倒排 + 阈值剪枝，快过 SpGEMM）

## 1.1 倒排索引（postings）数据结构：全用 NumPy（C 连续内存）

对每个年份 `y`，从 `My`（CSR）构建  **扁平倒排** ：

* `ptr_y`: int64/uint64，shape `(V+1,)`
* `docs_y`: int32，shape `(nnz_y,)`
* `vals_y`: float32，shape `(nnz_y,)`
* `max_y`: float32，shape `(V,)`，`max_y[w] = max vals for term w`（用于 MaxScore）

落盘缓存（减少重复构建）：

<pre class="overflow-visible! px-0!" data-start="1186" data-end="1343"><div class="contain-inline-size rounded-2xl corner-superellipse/1.1 relative bg-token-sidebar-surface-primary"><div class="sticky top-[calc(--spacing(9)+var(--header-height))] @w-xl/main:top-9"><div class="absolute end-0 bottom-0 flex h-9 items-center pe-2"><div class="bg-token-bg-elevated-secondary text-token-text-secondary flex items-center gap-4 rounded-sm px-2 font-sans text-xs"></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre!"><span><span>artifacts/postings/year=2012_ptr.npy
artifacts/postings/year=2012_docs.npy
artifacts/postings/year=2012_vals.npy
artifacts/postings/year=2012_max.npy
</span></span></code></div></div></pre>

> postings 访问是 O(1) slice：`start=ptr[w]; end=ptr[w+1]`，切片是 C 层视图。

## 1.2 term-at-a-time 累加：算的是一行 `score[j] = s(i,j)`

固定 `X` 中一个文档 `i`（CSR 行）：

* 取出该行 `terms = indices`, `xvals = data`
* 遍历每个 term `w`：
  * 取 `docs = docs_y[start:end]`, `yvals = vals_y[start:end]`
  * 对每个 `j in docs` 做：`score[j] += xw * yval`

## 1.3 阈值前移：MaxScore 无损剪枝（关键提速点）

对该行每个 term `w` 的上界贡献：

* `ub = xw * max_y[w]`

把 term 按 `ub` 降序排序（你已做 TopK，行内 term 数 ~30，排序成本很低）。

维护 `remain = sum(ub)`，处理一个 term 后 `remain -= ub_term`。

对候选 `j`：若当前 `score[j] + remain < thr`，则 `j`  **不可能达阈值** ，可立刻丢弃，后续 term 不再更新它（无损）。

> 这一步能把大量“最终达不到阈值”的候选提前杀掉，减少候选更新次数，通常带来数量级加速。

---

# 任务 2：分块运算（压内存，时间几乎不变甚至更快）

IR 模型逻辑上已经是“按行”，但工程上仍要按块组织外层循环以：

* 控制峰值内存/临时对象生命周期
* 方便进度日志与恢复
* 减少 swap 风险

## 2.1 推荐的分块方式：按 X 的行分块（doc blocks）

参数：`block_size_docs`（默认 10k，可调 5k~50k）

流程：

* 每次只处理 `i in [b, b+block)` 的文档
* `contrib_x` 写入对应片段
* block 完成输出日志（速度、命中数、候选规模统计）

## 2.2 postings 内存策略（避免 swap）

* **优先稳** ：`docs_y/vals_y` 使用 `np.memmap`（mmap），不把 1200w nnz 全塞进 RAM
* 机器够大再切换到内存常驻（更快）

---

# 任务 3：(x,y) 只算一次，同时更新两个方向（省一半时间）

相似度对称：`s(i,j)=s(j,i)`。

你在计算 `(x,y)` 时采用 “X 查询 Y” 的过程，得到所有 `j` 及其真实 `score[j]`，这时：

* `contrib_x[i] += score[j]`
* `contrib_y[j] += score[j]`

 **同一份 score 同时写两边** ，不需要再跑一次 `(y,x)`。

注意事项：

* 必须保证阈值规则一致（同一 `thr`）
* 如果你的 BS/FS 是前后窗口方向性的：

  方向性只体现在“汇总时加到 BS 还是 FS”，而不是 pair 计算本身

  pair 贡献仍可双向复用

---

# 任务 4：pair 级落盘 checkpoint（算完一个年份对就存档）

## 4.1 pair 任务列表生成（避免全 N^2）

如果你的窗口是 `±5` 年：

* 对每个 `t`：只需要 pairs `(t, t±1..5)`
* 去重后保存：

<pre class="overflow-visible! px-0!" data-start="2842" data-end="2920"><div class="contain-inline-size rounded-2xl corner-superellipse/1.1 relative bg-token-sidebar-surface-primary"><div class="sticky top-[calc(--spacing(9)+var(--header-height))] @w-xl/main:top-9"><div class="absolute end-0 bottom-0 flex h-9 items-center pe-2"><div class="bg-token-bg-elevated-secondary text-token-text-secondary flex items-center gap-4 rounded-sm px-2 font-sans text-xs"></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre!"><span><span>artifacts/pair_list</span><span>.json</span><span>
</span><span>[(2008,2012), (2008,2011), ...]</span><span>   # 建议统一存 </span><span>x</span><span><</span><span>y</span><span>
</span></span></code></div></div></pre>

## 4.2 断点续跑策略（文件存在即完成）

计算前检查：

* 若 `pair_contrib/x=..._y=....npz` 存在 → skip

写入采用原子策略：

* 写 `.tmp.npz` → `rename` 为正式 `.npz`

## 4.3 汇总阶段（BS/FS）变成纯 IO + 向量加法

对每年 t：

* BS(t)：累加 back_years 对应 pair 的 “t 那一侧贡献向量”
* FS(t)：累加 forward_years 对应 pair 的 “t 那一侧贡献向量”

这一步几乎不耗时。

---

# 性能实现要求：避开 Python 低效方案，用 C/NumPy/Numba

你要想真的比 SpGEMM 快 10×，重点是 **候选累加器（accumulator）** 的实现。

## 必须避免

* Python `dict` 在热循环里 `scores[j] += delta`
* Python 双层 for 循环更新上亿次候选（解释器会死）

## 推荐实现（强烈建议）：稀疏 accumulator + touched 列表（可 Numba 化）

对固定一行（一个文档 i）：

* `acc = np.zeros(Ny, float32)`  （可复用，不要每行 new）
* `mark = np.zeros(Ny, uint8)` 或 `seen = np.zeros(Ny, int32)`（可选）
* `touched = np.empty(max_touch, int32)` + `touched_len`（Numba 友好）

更新时（在 Numba 里）：

* 如果 `mark[j]==0`：`mark[j]=1; touched[touched_len]=j; touched_len+=1`
* `acc[j] += delta`

行结束后：

* 遍历 `touched[0:touched_len]`：
  * 若 `acc[j] >= thr`：更新 `contrib_x[i]` 与 `contrib_y[j]`
  * 清理：`acc[j]=0; mark[j]=0`
* `touched_len=0`

> 这样“acc 是 C 连续数组写入”，比 dict 快很多，并且 touched 通常远小于 Ny。

## MaxScore 剪枝的实现（仍建议放 Numba 内）

* 行内 term 数不大（你 TopK ~30），排序可在 Python/NumPy 侧完成
* 剪枝时对 touched 候选做条件删除，需要小心实现方式：
  * 简化版本：不做“中途删除”，只在行末筛 `>=thr`（正确但剪枝弱）
  * 完整版本：维护“活跃候选列表”，根据 `acc[j]+remain<thr` 逐步剔除（更快）
* 建议推进顺序：
  1. 先实现无剪枝版本（结果一致）
  2. 再加 MaxScore 剪枝（加速）

## dtype 约束（强制）

* doc_id：int32
* 权重：float32
* contrib：float32
* postings ptr：int64/uint64（长度 V+1，安全）

## postings 构建成本优化

* postings 只构建一次并缓存（或 mmap）
* 不要每个 pair 重建
* 优先将 My 转 CSC 来构建 postings（列访问自然），构建完再存扁平 arrays

---

# 最终主流程（你要实现的核心 pipeline 行为）

## 预处理（每个年份 y，一次性）

1. 读取 `vectors_filtered/year=y.npz`（CSR）
2. 构建 postings：`ptr_y, docs_y, vals_y, max_y`
3. 保存到 `artifacts/postings/`

## 主计算（遍历 pair_list，只算一次 x<y）

对每个 pair `(x,y)`：

1. 若 `pair_contrib/x=..._y=....npz` 存在：continue
2. 加载 `Mx`（CSR）
3. mmap/加载 `postings(y)`（ptr/docs/vals/max）
4. 分配：
   * `contrib_x = zeros(Nx,float32)`
   * `contrib_y = zeros(Ny,float32)`
   * 复用 `acc, mark, touched`（按块或按线程）
5. 对 x 的文档按 block 处理：
   * 对 block 内每个 i：
     * 读取行的 term/weight
     * （可选）按 `ub=xw*max_y[w]` 排序
     * term-at-a-time 累加到 acc
     * 行末遍历 touched：筛 `acc[j]>=thr`，双向更新 contrib，并清理
6. 原子写入 pair_contrib 文件

## 汇总（BS/FS）

对每年 t：

* 读对应 window 的 pair_contrib，取“t 那一侧的 contrib”向量相加
* 输出 bs/fs（以及最终专利指标所需的后续统计）

---

# 关键注意事项（避免踩坑）

1. **不要构造 S** ：任何形式的 `Mx.dot(My.T)` 都会回到原瓶颈
2. **避免 Python dict 做 scores** ：可先用于小规模验证，最终必须替换
3. **保证 doc 顺序一致** ：同一年份的文档索引必须在所有阶段保持一致，否则 contrib 无法对齐
4. **pair 写入要原子** ：否则断点续跑会读到坏文件
5. **method_version** ：算法参数（thr、TopK、df 剪枝、MaxScore 开关）变化后应使旧 pair 结果失效（可通过目录名或 meta 校验）

---

如果你愿意，我可以把这个方案进一步“工程化到你仓库结构”——给出建议的文件拆分与函数签名（例如 `postings.py`, `pair_compute.py`, `aggregate.py`），以及每个模块应该记录哪些日志字段、checkpoint 如何接到你现有 pipeline 的 cascade reset 逻辑里。
